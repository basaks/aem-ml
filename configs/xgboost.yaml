data:
    aem_folder: 'aem_data'
    train_data:
        aem_train_data:
            - 'high_res_cond_clip_albers_skip_6.shp'
        targets:
            - 'interpretation_zone53_albers_study_area_Ceno_depth.shp'
        weights:
            - 1
    apply_model: 'oss.shp'
    weight_col: 'BoundConf'
    line_col: 'SURVEY_LIN'
    conductivity_columns_prefix: 'cond'
    thickness_columns_prefix: 'thick'
    aem_covariate_cols:
        - 'ceno_euc_a'
        - 'Gravity_la'
        - 'national_W'
        - 'relief_ele'
        - 'relief_mrv'
        - 'SagaWET9ce'
        - 'elevation'
        - 'tx_height'
    # aem line scan radius in meters - aem points are assumed to be on the same flight line under this radius
    aem_line_scan_radius: 500
    # aem lines are split into batches of aem_line_splits's to generate artificial splits
    aem_line_splits: 1000
    test_train_split:
        train: 0.6
        val: 0.2
        test: 0.2
    rows: -1

# will run n_iter * cv number of jobs
# n_iter: Number of parameter settings that are sampled.
#n_jobs: int, default=1
#    Number of jobs to run in parallel. At maximum there are
#    ``n_points`` times ``cv`` jobs available during each iteration.
#
#n_points: int, default=1
#    Number of parameter settings to sample in parallel. If this does
#    not align with ``n_iter``, the last iteration will sample less
#    points.

learning:
    algorithm: xgboost
    params:
        objective: 'reg:squarederror'
        max_depth: 15
        learning_rate: 0.23
        n_estimators: 200
        colsample_bynode: 0.8
        colsample_bylevel: 1.0
        colsample_bytree: 0.9
        max_delta_step: 9
        gamma: 0.5
        reg_alpha: 57.96
        reg_lambda: 10.0
        subsample: 1.0
    cross_validation:
        kfold: 3
    weighted_model:
        weights:
            H: 2
            M: 1
            L: 0.5
    numpy_seed: 10
    include_aem_covariates: true
    include_thickness: true
    include_conductivity_derivatives: true
    smooth_twod_covariates: true
    smooth_covariates_kernel_size: (21, 9)
    # depth - columns (x) -- 30 sections
    # along the line is - rows (y) -- many 100s of kms with resolutionof 12meters
    optimisation:
        searchcv_params:
            n_iter: 6
            cv: 2
            verbose: 1000
            n_points: 3
            n_jobs: 6
        params_space:
            'max_depth': Integer(1, 15)
            'learning_rate': Real(10 ** -5, 10 ** 0, prior="log-uniform")
            'n_estimators': Integer(2, 20)
            'min_child_weight': Integer(1, 10)
            'max_delta_step': Integer(0, 10)
            'gamma': Real(0, 0.5, prior="uniform")
            'colsample_bytree': Real(0.3, 0.9, prior="uniform")
            'subsample': Real(0.01, 1.0, prior='uniform')
            'colsample_bylevel': Real(0.01, 1.0, prior='uniform')
            'colsample_bynode': Real(0.01, 1.0, prior='uniform')
            'reg_alpha': Real(1, 100, prior='uniform')
            'reg_lambda': Real(0.01, 10, prior='log-uniform')


output:
    directory: out/xgboost/
    train:
        covariates_csv: true
        true_vs_pred: true
        feature_ranking: true
    pred:
        quantiles: 0.95
        optimised_model: true
        covariates_csv: true
        pred: true
